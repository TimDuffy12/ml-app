{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability, Conditional Probability, Bayes' Theorem\n",
    "## Conditional Probability\n",
    "**Conditional Probability is defined as the probability of an event ( A ), given that another ( B ) has already occurred.**\n",
    "\n",
    "If events A and B are not independent, then the probability of the intersection of A and B (the probability that both P(B|A) = vents occur) is defined by \n",
    "P(A and B) = P(A)P(B|A).\n",
    "\n",
    "From this definition, the conditional probability P(B|A) is easily obtained by dividing by P(A):\n",
    "\n",
    "**P(B|A) = P(B and A) / P(A)**\n",
    "\n",
    "In the Predictive Analytics section we will learn a very widely used **Classification** algorithm called the **Naive Bayes Classifiaction Algorithm**.\n",
    "\n",
    "It is a Machine Learning algorithm that is often used in data sets with multiple attributes. It is very easy to calculate and hence is often used to classify things in real time, such as \"if an email containing a set of key words is classified as spam\", \"a newly published article belongs to a class of articles\", \"if an insurance claim, just submitted is real or fraud\" etc.\n",
    "\n",
    "The **Bayes** part of the name comes from Thomas Bayes, the inventor of the foundational Bayes' theorem and the **Naive** part of the name comes from the assumption that the factors guiding the occurrance of an event are **independent** of each other, even though in real life, they may not be so (a somewhat **naive** assumption). However, this algorithm produces very good/reliable results and is widely used.\n",
    "\n",
    "\n",
    "\n",
    "## Bayes' Theorem\n",
    "Bayes' Theorem (also called Bayes' Law or Bayes' Formula) is stated as\n",
    "\n",
    "***Probability of an event A given that an event B has occurred, is equal to the probability of B given A has occurred multiplied by the probability of A given B has occurred divided by the probability of B***\n",
    "\n",
    "***P(A|B) = (P(B|A) X P(A))/P(B)***\n",
    "\n",
    "where\n",
    "\n",
    "P(A|B) = Probability of event A given the event B has occurred\n",
    "\n",
    "P(B|A) = Probability of event B given the event A has occurred\n",
    "\n",
    "P(A), P(B) = Probabilities of event A and B respectively\n",
    "\n",
    "### Commonly used terms in Bayesian Classification\n",
    "A is called the **Proposition** and B is called the **Evidence**\n",
    "\n",
    "P(A) is called the **Prior Probability of Proposition** and P(B) is called the **Prior probability of Evidence**\n",
    "\n",
    "P(A|B) is called the **Posterior**\n",
    "\n",
    "P(B|A) is called the **Likelyhood**\n",
    "\n",
    "\n",
    "In other words\n",
    "\n",
    "***Posterior = (Likelihood X Prior Probability of Proposition)/Prior Probability of Evidence***\n",
    "\n",
    "### Bayesian Theorem as applied to Naive Bayes Algorithm\n",
    "In Machine Learning classification there are multiple clesses C1, C2, C3...and each class with multiple features x1, x2, x3...(e.g. an insurance claim is in class 'Valid' or 'Fraud' and each claim has features such as 'amount of claim', 'doctor submitting the claim', 'amount of the claim', 'frequency of high value claim for same treatment by the same doctor' etc.). The aim of the algorithm is to determine the **Conditional Probability** of an object (an insurance claim) with features x1, x2,...xn belonging to a class Ci.\n",
    "\n",
    "We will learn Bayesin Classification and it's calculation (using Python) in much more details in the **Predictive Analytics** section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=sjUDlJfdnKM.\n",
    "\n",
    "https://www.youtube.com/watch?v=CPqOCI0ahss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math as mth\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing K_Mean Clustering, Naive Bayes and Logistic Regression using Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris.feature_names\n",
    "iris.data\n",
    "iris.target_names\n",
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal Length</th>\n",
       "      <th>Sepal Width</th>\n",
       "      <th>Petal Length</th>\n",
       "      <th>Petal Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sepal Length  Sepal Width  Petal Length  Petal Width\n",
       "0            5.1          3.5           1.4          0.2\n",
       "1            4.9          3.0           1.4          0.2\n",
       "2            4.7          3.2           1.3          0.2\n",
       "3            4.6          3.1           1.5          0.2\n",
       "4            5.0          3.6           1.4          0.2\n",
       "5            5.4          3.9           1.7          0.4\n",
       "6            4.6          3.4           1.4          0.3\n",
       "7            5.0          3.4           1.5          0.2\n",
       "8            4.4          2.9           1.4          0.2\n",
       "9            4.9          3.1           1.5          0.1\n",
       "10           5.4          3.7           1.5          0.2\n",
       "11           4.8          3.4           1.6          0.2\n",
       "12           4.8          3.0           1.4          0.1\n",
       "13           4.3          3.0           1.1          0.1\n",
       "14           5.8          4.0           1.2          0.2\n",
       "15           5.7          4.4           1.5          0.4\n",
       "16           5.4          3.9           1.3          0.4\n",
       "17           5.1          3.5           1.4          0.3\n",
       "18           5.7          3.8           1.7          0.3\n",
       "19           5.1          3.8           1.5          0.3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = pd.DataFrame(iris.data)\n",
    "iris_df.columns = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "iris_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  0\n",
       "2  0\n",
       "3  0\n",
       "4  0\n",
       "5  0\n",
       "6  0\n",
       "7  0\n",
       "8  0\n",
       "9  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "140  2\n",
       "141  2\n",
       "142  2\n",
       "143  2\n",
       "144  2\n",
       "145  2\n",
       "146  2\n",
       "147  2\n",
       "148  2\n",
       "149  2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "30   0\n",
       "31   0\n",
       "32   0\n",
       "33   0\n",
       "34   0\n",
       "..  ..\n",
       "100  2\n",
       "101  2\n",
       "102  2\n",
       "103  2\n",
       "104  2\n",
       "\n",
       "[75 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df1 = pd.DataFrame(iris.target)\n",
    "iris_df1.head(10)\n",
    "iris_df1.tail(10)\n",
    "iris_df1[30:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using K-Mean Clustering\n",
    "\n",
    "## Defintions of \"Accuracy\", \"Precision\" and \"Recall\" scores of the prediction of a model\n",
    "\n",
    "### Accuracy = Number of correct prediction / Total Number of Predition\n",
    "\n",
    "### Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "### Recall = True Positive / (True Positive + False Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n",
      " 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n",
      " 2 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        50\n",
      "           1       0.00      0.00      0.00        50\n",
      "           2       0.95      0.72      0.82        50\n",
      "\n",
      "    accuracy                           0.24       150\n",
      "   macro avg       0.32      0.24      0.27       150\n",
      "weighted avg       0.32      0.24      0.27       150\n",
      "\n",
      "[[ 0 50  0]\n",
      " [48  0  2]\n",
      " [14  0 36]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmodel = kmeans.fit(iris.data)\n",
    "expected = iris.target\n",
    "#predicted = kmodel.labels_\n",
    "predicted = kmodel.predict(iris.data)\n",
    "print('======>', predicted)\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(expected, kmodel.labels_))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Gaussian Naive Bayes Model\n",
    "\n",
    "## Defintions of \"Accuracy\", \"Precision\" and \"Recall\" scores of the prediction of a model\n",
    "\n",
    "### Accuracy = Number of correct prediction / Total Number of Predition\n",
    "\n",
    "### Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "### Recall = True Positive / (True Positive + False Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_target = pd.DataFrame(iris.target)\n",
    "iris_target.columns = ['Target Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbmodel = GaussianNB()\n",
    "nbmodel.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = iris.target\n",
    "predicted = nbmodel.predict(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       0.94      0.94      0.94        50\n",
      "           2       0.94      0.94      0.94        50\n",
      "\n",
      "    accuracy                           0.96       150\n",
      "   macro avg       0.96      0.96      0.96       150\n",
      "weighted avg       0.96      0.96      0.96       150\n",
      "\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  3 47]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using  Multi-factors Logistic Regression\n",
    "\n",
    "## Defintions of \"Accuracy\", \"Precision\" and \"Recall\" scores of the prediction of a model\n",
    "\n",
    "### Accuracy = Number of correct prediction / Total Number of Predition\n",
    "\n",
    "### Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "### Recall = True Positive / (True Positive + False Negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "#print(metrics.classification_report(expected, predicted))\n",
    "#print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Result ==================\n",
      "++++++++++++++++ Actual/Expected ++++++++++++++++++\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "++++++++++++++ Predicted ++++++++++++++++++++++++++\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "=============== Model Performance Results ===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       0.98      0.90      0.94        50\n",
      "           2       0.91      0.98      0.94        50\n",
      "\n",
      "    accuracy                           0.96       150\n",
      "   macro avg       0.96      0.96      0.96       150\n",
      "weighted avg       0.96      0.96      0.96       150\n",
      "\n",
      "============ Model Confusion Matrix ===========\n",
      "[[50  0  0]\n",
      " [ 0 45  5]\n",
      " [ 0  1 49]]\n",
      "=============== Model Accuracy ==============\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(iris.data, iris.target)\n",
    "#print(lr_model)\n",
    "# make predictions\n",
    "expected = iris.target\n",
    "predicted = lr_model.predict(iris.data)\n",
    "\n",
    "print('================ Result ==================')\n",
    "print('++++++++++++++++ Actual/Expected ++++++++++++++++++')\n",
    "print(expected)\n",
    "print('++++++++++++++ Predicted ++++++++++++++++++++++++++')\n",
    "print(predicted)\n",
    "# summarize the fit of the model\n",
    "print('=============== Model Performance Results ===========')\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print('============ Model Confusion Matrix ===========')\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "print('=============== Model Accuracy ==============')\n",
    "print(metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Naive Bayes and Multi-Class Regression classifiers Native Indian Diabetes data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defintions of \"Accuracy\", \"Precision\" and \"Recall\" scores of the prediction of a model\n",
    "\n",
    "### Accuracy = Number of correct prediction / Total Number of Predition\n",
    "\n",
    "### Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "### Recall = True Positive / (True Positive + False Negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv(\"../data/pima-indians-diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodP</th>\n",
       "      <th>SkinThick</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pregnancies  Glucose  BloodP  SkinThick  Insulin   BMI  \\\n",
       "0             6      148      72         35        0  33.6   \n",
       "1             1       85      66         29        0  26.6   \n",
       "2             8      183      64          0        0  23.3   \n",
       "3             1       89      66         23       94  28.1   \n",
       "4             0      137      40         35      168  43.1   \n",
       "5             5      116      74          0        0  25.6   \n",
       "6             3       78      50         32       88  31.0   \n",
       "7            10      115       0          0        0  35.3   \n",
       "8             2      197      70         45      543  30.5   \n",
       "9             8      125      96          0        0   0.0   \n",
       "10            4      110      92          0        0  37.6   \n",
       "11           10      168      74          0        0  38.0   \n",
       "12           10      139      80          0        0  27.1   \n",
       "13            1      189      60         23      846  30.1   \n",
       "14            5      166      72         19      175  25.8   \n",
       "15            7      100       0          0        0  30.0   \n",
       "16            0      118      84         47      230  45.8   \n",
       "17            7      107      74          0        0  29.6   \n",
       "18            1      103      30         38       83  43.3   \n",
       "19            1      115      70         30       96  34.6   \n",
       "\n",
       "    DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                      0.627   50        1  \n",
       "1                      0.351   31        0  \n",
       "2                      0.672   32        1  \n",
       "3                      0.167   21        0  \n",
       "4                      2.288   33        1  \n",
       "5                      0.201   30        0  \n",
       "6                      0.248   26        1  \n",
       "7                      0.134   29        0  \n",
       "8                      0.158   53        1  \n",
       "9                      0.232   54        1  \n",
       "10                     0.191   30        0  \n",
       "11                     0.537   34        1  \n",
       "12                     1.441   57        0  \n",
       "13                     0.398   59        1  \n",
       "14                     0.587   51        1  \n",
       "15                     0.484   32        1  \n",
       "16                     0.551   31        1  \n",
       "17                     0.254   31        1  \n",
       "18                     0.183   33        0  \n",
       "19                     0.529   32        1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodP</th>\n",
       "      <th>SkinThick</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose      BloodP   SkinThick     Insulin  \\\n",
       "count   768.000000  768.000000  768.000000  768.000000  768.000000   \n",
       "mean      3.845052  120.894531   69.105469   20.536458   79.799479   \n",
       "std       3.369578   31.972618   19.355807   15.952218  115.244002   \n",
       "min       0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%       1.000000   99.000000   62.000000    0.000000    0.000000   \n",
       "50%       3.000000  117.000000   72.000000   23.000000   30.500000   \n",
       "75%       6.000000  140.250000   80.000000   32.000000  127.250000   \n",
       "max      17.000000  199.000000  122.000000   99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB as gnb\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"]=15, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodP</th>\n",
       "      <th>SkinThick</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>88</td>\n",
       "      <td>41</td>\n",
       "      <td>235</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.704</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>99</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.388</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>196</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.451</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pregnancies  Glucose  BloodP  SkinThick  Insulin   BMI  \\\n",
       "0             6      148      72         35        0  33.6   \n",
       "1             1       85      66         29        0  26.6   \n",
       "2             8      183      64          0        0  23.3   \n",
       "3             1       89      66         23       94  28.1   \n",
       "4             0      137      40         35      168  43.1   \n",
       "5             5      116      74          0        0  25.6   \n",
       "6             3       78      50         32       88  31.0   \n",
       "8             2      197      70         45      543  30.5   \n",
       "10            4      110      92          0        0  37.6   \n",
       "11           10      168      74          0        0  38.0   \n",
       "12           10      139      80          0        0  27.1   \n",
       "13            1      189      60         23      846  30.1   \n",
       "14            5      166      72         19      175  25.8   \n",
       "16            0      118      84         47      230  45.8   \n",
       "17            7      107      74          0        0  29.6   \n",
       "18            1      103      30         38       83  43.3   \n",
       "19            1      115      70         30       96  34.6   \n",
       "20            3      126      88         41      235  39.3   \n",
       "21            8       99      84          0        0  35.4   \n",
       "22            7      196      90          0        0  39.8   \n",
       "\n",
       "    DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                      0.627   50        1  \n",
       "1                      0.351   31        0  \n",
       "2                      0.672   32        1  \n",
       "3                      0.167   21        0  \n",
       "4                      2.288   33        1  \n",
       "5                      0.201   30        0  \n",
       "6                      0.248   26        1  \n",
       "8                      0.158   53        1  \n",
       "10                     0.191   30        0  \n",
       "11                     0.537   34        1  \n",
       "12                     1.441   57        0  \n",
       "13                     0.398   59        1  \n",
       "14                     0.587   51        1  \n",
       "16                     0.551   31        1  \n",
       "17                     0.254   31        1  \n",
       "18                     0.183   33        0  \n",
       "19                     0.529   32        1  \n",
       "20                     0.704   27        0  \n",
       "21                     0.388   50        0  \n",
       "22                     0.451   41        1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(724, 9)\n",
      "(543, 9)\n",
      "(181, 9)\n"
     ]
    }
   ],
   "source": [
    "data_mod = df0[(df0.BloodP != 0) & (df0.BMI != 0) & (df0.Glucose != 0)]\n",
    "data_mod.head(20)\n",
    "train, test = train_test_split(data_mod, test_size=0.25)\n",
    "print(data_mod.shape)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes  mean accuracy:  75.867 % std:  0.092 %\n",
      "Logistic Regression  mean accuracy:  76.591 % std:  0.135 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "features = ['Pregnancies', 'Glucose', 'BloodP', 'SkinThick', 'BMI', 'Age', 'Insulin', 'DiabetesPedigreeFunction']\n",
    "target = 'Outcome'\n",
    "\n",
    "classifiers = [gnb(), lr()]\n",
    "\n",
    "classifier_names = ['Gaussian Naive Bayes', 'Logistic Regression']\n",
    "\n",
    "for clf, clf_name in zip(classifiers, classifier_names):\n",
    "    cv_scores = cross_val_score(clf, train[features], train[target], cv=5)\n",
    "    \n",
    "    print(clf_name, ' mean accuracy: ', round(cv_scores.mean()*100, 3), '% std: ', round(cv_scores.var()*100, 3),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the X and Y axes of the Heat Map\n",
    "\n",
    "### X-Axis = Prediction, 0 = True 1 = False\n",
    "### Y-Axis = Actual, 0 = True, 1 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy for Gaussian naive bayes classifier: 73.48 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       121\n",
      "           1       0.61      0.55      0.58        60\n",
      "\n",
      "    accuracy                           0.73       181\n",
      "   macro avg       0.70      0.69      0.69       181\n",
      "weighted avg       0.73      0.73      0.73       181\n",
      "\n",
      "[[100  21]\n",
      " [ 27  33]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'confusion matrix for Gaussian naive bayes classifier')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1bd69e0abc8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_model_gnb = gnb().fit(train[features], train[target])\n",
    "y_hat_gnb = final_model_gnb.predict(test[features])\n",
    "\n",
    "print('test accuracy for Gaussian naive bayes classifier:', \\\n",
    "      round(accuracy_score(test[target], y_hat_gnb)*100, 2),'%')\n",
    "print(metrics.classification_report(test[target], y_hat_gnb))\n",
    "print(confusion_matrix(test[target], y_hat_gnb))\n",
    "plt.title('confusion matrix for Gaussian naive bayes classifier')\n",
    "sns.heatmap(confusion_matrix(test[target], y_hat_gnb), annot=True, cmap=\"YlGn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy for Logistic Regression classifier: 73.48 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'confusion matrix for Logistic Regression classifier')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.88      0.82       121\n",
      "           1       0.64      0.45      0.53        60\n",
      "\n",
      "    accuracy                           0.73       181\n",
      "   macro avg       0.70      0.66      0.67       181\n",
      "weighted avg       0.72      0.73      0.72       181\n",
      "\n",
      "[[106  15]\n",
      " [ 33  27]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1bd6a25d888>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_model_lr = lr().fit(train[features], train[target])\n",
    "y_hat_lr = final_model_lr.predict(test[features])\n",
    "\n",
    "print('test accuracy for Logistic Regression classifier:', \\\n",
    "      round(accuracy_score(test[target], y_hat_lr)*100, 2),'%')\n",
    "plt.title('confusion matrix for Logistic Regression classifier')\n",
    "print(metrics.classification_report(test[target], y_hat_lr))\n",
    "print(confusion_matrix(test[target], y_hat_lr))\n",
    "sns.heatmap(confusion_matrix(test[target], y_hat_lr), annot=True, cmap=\"YlGn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
